{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2b26f5-9a42-4442-897d-5389199d410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task to carry out: Let’s expand the maze to 2D with the 3 × 3 configuration shown in the figure \n",
    "above. State 9 (shaded in green) is our  ideal state or exit state. In other words, it has the \n",
    "highest reward. State 4 (shaded in red) has a trap. Going there  will incur a big penalty (you \n",
    "can decide what that is, but you want to pick a negative number). Try to find the  shortest path \n",
    "from a start state to the end state using the RL technique that we used in the RL_Example. Your \n",
    "output should  be a matrix with appropriate dimensions that one can use to traverse through this \n",
    "maze and get to the exit state. \n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# hyperparameters\n",
    "SMALL_ENOUGH = 0.05 # anything smaller never finishes running for my machine\n",
    "GAMMA = 0.9         \n",
    "NOISE = 0.1 \n",
    "\n",
    "# define all states\n",
    "all_states = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# define rewards for all states\n",
    "rewards = {}\n",
    "for val in all_states:\n",
    "    if val == 4:\n",
    "        rewards[val] = -8\n",
    "    elif val == 9:\n",
    "        rewards[val] = 10\n",
    "    else:\n",
    "        rewards[val] = 0\n",
    "\n",
    "# dictionary of possible actions going by the position radius (including the position itself)\n",
    "actions = {\n",
    "    1:(1, 2, 4),\n",
    "    2:(1, 2, 3, 5),\n",
    "    3:(2, 3, 6),\n",
    "    4:(1, 4, 5, 7),\n",
    "    5:(2, 4, 5, 6, 8),\n",
    "    6:(3, 5, 6, 9),\n",
    "    7:(4, 7, 8),\n",
    "    8:(5, 7, 8, 9)\n",
    "}\n",
    "\n",
    "# define an initial policy\n",
    "policy={}\n",
    "policy[1] = 2\n",
    "policy[2] = 3\n",
    "policy[3] = 6\n",
    "policy[4] = 7\n",
    "policy[5] = 6\n",
    "policy[6] = 9\n",
    "policy[7] = 8\n",
    "policy[8] = 9\n",
    "\n",
    "# define the initial value function\n",
    "V={}\n",
    "V[1] = 0\n",
    "V[6] = 0\n",
    "V[2] = 0\n",
    "V[3] = 0\n",
    "V[4] = 0\n",
    "V[5] = 0\n",
    "V[7] = 0\n",
    "V[8] = 0\n",
    "V[9] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851953cb-1458-4c27-8223-2d935487fd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: {1: 0, 6: 0, 2: 0, 3: 0, 4: 0, 5: 0, 7: 0, 8: 0, 9: 0}\n",
      "policy: {1: 2, 2: 3, 3: 6, 4: 7, 5: 6, 6: 9, 7: 8, 8: 9}\n"
     ]
    }
   ],
   "source": [
    "# value iteration\n",
    "iteration = 0  \n",
    "\n",
    "while True:\n",
    "    biggest_change = 0  \n",
    "    for s in all_states:  \n",
    "        if s in policy:  \n",
    "            old_v = V[s]  \n",
    "            new_v = 0  \n",
    "            \n",
    "            for a in actions[s]: \n",
    "                next = a \n",
    "                act = np.random.choice([i for i in actions[s]])\n",
    "                v = rewards[s] + (GAMMA * ((1 - NOISE) * V[next] + (NOISE * V[act])))\n",
    "                if v > new_v:  \n",
    "                    new_v = v  \n",
    "                    policy[s] = a \n",
    " \n",
    "            V[s] = new_v\n",
    "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "         \n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "        break \n",
    "    iteration += 1  \n",
    "\n",
    "# print the final value function and policy\n",
    "print('V:', V) \n",
    "print('policy:', policy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdda6b08-a0bd-4129-a084-775741fad160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results:\n",
    "#\n",
    "# As seen through the policy, we can use this route for any location we start from to successfully \n",
    "# end up in the state of 9. The only time we are ever in the state of 4 (the trap) is if we ever \n",
    "# have to start in that state. Otherwise, through this reinforcement learning technique, we can \n",
    "# stay away from areas that have low rewards and learn to find the \"correct\" route to go through \n",
    "# the process of rewards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
