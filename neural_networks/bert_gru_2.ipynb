{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3240bcc9-2e0a-493e-acf9-cf1fb77af258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.712 | Valid Loss: 0.795 | Valid Acc: 50.00% | Time: 6.04s\n",
      "Epoch 2 | Train Loss: 0.276 | Valid Loss: 0.997 | Valid Acc: 50.00% | Time: 6.73s\n",
      "Epoch 3 | Train Loss: 0.129 | Valid Loss: 1.255 | Valid Acc: 50.00% | Time: 7.75s\n",
      "Epoch 4 | Train Loss: 0.069 | Valid Loss: 1.517 | Valid Acc: 50.00% | Time: 7.54s\n",
      "Epoch 5 | Train Loss: 0.019 | Valid Loss: 1.756 | Valid Acc: 50.00% | Time: 6.75s\n",
      "Test Loss: 1.100 | Test Acc: 0.00%\n",
      "('Negative', 0.35799726843833923)\n",
      "('Negative', 0.32282501459121704)\n"
     ]
    }
   ],
   "source": [
    "#Task 4\n",
    "\"\"\"\n",
    "This program demonstrates a binary sentiment classification system using a combination of a pre-trained BERT model \n",
    "and a GRU layer. The BERT model extracts contextual embeddings from text, which are passed through a GRU layer \n",
    "to learn sequential patterns. A fully connected layer is then used to classify the sentiment as either positive or negative.\n",
    "\n",
    "The code covers the following steps:\n",
    "1. Dataset Preparation: Example text data is tokenized using the BERT tokenizer and split into training, validation, and test sets.\n",
    "2. Model Design: A BERT-GRU model is implemented, where the GRU layer processes embeddings from the BERT model.\n",
    "3. Training: The model is trained for 5 epochs using binary cross-entropy loss and the Adam optimizer, with the best model saved.\n",
    "4. Evaluation: The model is evaluated on validation and test datasets, calculating loss and accuracy.\n",
    "5. Prediction: A function is provided to predict the sentiment of new text inputs, outputting both the sentiment and confidence score.\n",
    "\n",
    "This code serves as an introduction to using transformers with RNNs for text classification tasks.\n",
    "\"\"\"\n",
    "\n",
    "import torch  # PyTorch library for deep learning\n",
    "from torch.utils.data import DataLoader, Dataset  # Classes for handling datasets and batching\n",
    "import torch.nn as nn  # Neural network module\n",
    "import torch.optim as optim  # Optimizers for training\n",
    "from transformers import BertTokenizer, BertModel  # Pre-trained BERT tokenizer and model\n",
    "from sklearn.model_selection import train_test_split  # Split data into training, validation, and test sets\n",
    "import time  # To measure training time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Dataset preparation\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        # Store the input texts and their labels\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve a text and its label by index\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text and return input IDs, attention mask, and label\n",
    "        tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        return tokens['input_ids'].squeeze(0), tokens['attention_mask'].squeeze(0), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# Example dataset\n",
    "texts = [\"I love this movie!\", \"This is terrible.\", \"Fantastic work.\", \"Horrible experience.\", \"Amazing product.\"]  # Example texts\n",
    "labels = [1, 0, 1, 0, 1]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Train-test split\n",
    "# Split data into training, validation, and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.4, random_state=SEED)\n",
    "train_texts, valid_texts, train_labels, valid_labels = train_test_split(train_texts, train_labels, test_size=0.5, random_state=SEED)\n",
    "\n",
    "# Create dataset objects for each split\n",
    "train_dataset = TextDataset(train_texts, train_labels)\n",
    "valid_dataset = TextDataset(valid_texts, valid_labels)\n",
    "test_dataset = TextDataset(test_texts, test_labels)\n",
    "\n",
    "BATCH_SIZE = 2  # Number of samples per batch\n",
    "\n",
    "# Create DataLoaders to batch and shuffle the data\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define the BERT-GRU model\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert  # Pre-trained BERT model\n",
    "        self.rnn = nn.GRU(bert.config.hidden_size, hidden_dim, batch_first=True, bidirectional=True)  # GRU layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Fully connected layer for binary classification\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass input through BERT and extract embeddings\n",
    "        with torch.no_grad():  # Freeze BERT weights\n",
    "            embedded = self.bert(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        # Pass embeddings through GRU\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        # Concatenate the last forward and backward GRU hidden states\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        return self.fc(hidden)  # Pass through the fully connected layer\n",
    "\n",
    "# Initialize the BERT-GRU model\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = BERTGRUSentiment(bert, hidden_dim=128, output_dim=1, dropout=0.3).to(torch.device(\"cpu\"))\n",
    "\n",
    "# Freeze BERT parameters to avoid updating them during training\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss\n",
    "optimizer = optim.Adam(model.parameters())  # Adam optimizer\n",
    "\n",
    "# Define helper functions\n",
    "def binary_accuracy(preds, y):\n",
    "    # Calculate accuracy by comparing predictions to true labels\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    # Train the model for one epoch\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for input_ids, attention_mask, labels in loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(input_ids, attention_mask).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "def evaluate_with_accuracy(model, loader, criterion):\n",
    "    # Evaluate the model and calculate accuracy\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in loader:\n",
    "            predictions = model(input_ids, attention_mask).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(predictions) > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return epoch_loss / len(loader), correct / total\n",
    "\n",
    "# Training loop\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_with_accuracy(model, valid_loader, criterion)\n",
    "    end_time = time.time()\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'bert_gru_model.pt')  # Save the best model\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}% | Time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load('bert_gru_model.pt'))\n",
    "test_loss, test_acc = evaluate_with_accuracy(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Prediction function\n",
    "def predict_sentiment(model, tokenizer, text):\n",
    "    # Predict sentiment for a single input text\n",
    "    tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.sigmoid(model(input_ids, attention_mask)).item()\n",
    "    return \"Positive\" if prediction > 0.5 else \"Negative\", prediction\n",
    "\n",
    "# Example predictions\n",
    "print(predict_sentiment(model, tokenizer, \"I absolutely love this!\"))\n",
    "print(predict_sentiment(model, tokenizer, \"This was a horrible experience.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b6c5d-8249-45d1-8b87-9d288901310e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
