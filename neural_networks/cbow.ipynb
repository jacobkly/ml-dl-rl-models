{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c00d31d-15c6-4cfd-a3e0-c90094a38f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10388.5395\n",
      "Epoch 2, Loss: 10384.4899\n",
      "Epoch 3, Loss: 10380.4990\n",
      "Epoch 4, Loss: 10376.6682\n",
      "Epoch 5, Loss: 10372.8964\n",
      "Context: ['Current events of September\\xa03,\\xa01995\\xa0(1995-09-03) (Sunday) :', 'eBay is founded.', 'Current events of September\\xa06,\\xa01995\\xa0(1995-09-06) (Wednesday) :', 'NATO air strikes against Bosnian Serb forces continue, after repeated attempts at a solution to the Bosnian War fail.']\n",
      "Predicted word: Syrian civil war:\n"
     ]
    }
   ],
   "source": [
    "# This code implements a Continuous Bag of Words (CBOW) model using PyTorch to predict a target word based on its context words. \n",
    "# It reads a text dataset, preprocesses it, defines the model, and trains it to learn word embeddings and predict words efficiently.\n",
    "\n",
    "# Import required libraries\n",
    "import torch  # Library for machine learning and deep learning\n",
    "import torch.nn as nn  # Module for building neural networks\n",
    "from torch.utils.data import Dataset, DataLoader  # Tools for handling datasets and creating data loaders\n",
    "\n",
    "# Define hyperparameters for the model\n",
    "CONTEXT_SIZE = 2  # Number of words on each side of the target word (context window size)\n",
    "EMBEDDING_DIM = 100  # Size of word embeddings (vector representation of words)\n",
    "BATCH_SIZE = 32  # Number of samples per batch for training\n",
    "EPOCHS = 5  # Number of times to go through the entire dataset during training\n",
    "LEARNING_RATE = 0.01  # Learning rate for the optimizer (how big the steps are for updating model weights)\n",
    "\n",
    "# Load the dataset from a text file\n",
    "with open('news_corpus.txt', 'r') as f:  # Open the file in read mode\n",
    "    raw_text = f.read().splitlines()  # Read all lines and split them into a list of strings\n",
    "\n",
    "# Create a vocabulary from the dataset\n",
    "vocab = set(raw_text)  # Get unique words from the dataset\n",
    "vocab_size = len(vocab)  # Count the number of unique words\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}  # Map each word to a unique index\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}  # Map each index back to its word\n",
    "\n",
    "# Define a dataset class for CBOW\n",
    "class CBOWDataset(Dataset):  # Inherit from PyTorch's Dataset class\n",
    "    def __init__(self, text, context_size):  # Initialize the dataset\n",
    "        self.data = []  # List to hold (context, target) pairs\n",
    "        for i in range(context_size, len(text) - context_size):  # Iterate over text with room for context\n",
    "            context = (  # Collect context words (words before and after the target)\n",
    "                text[i - context_size:i] + text[i + 1:i + 1 + context_size]\n",
    "            )\n",
    "            target = text[i]  # The target word is the word in the middle\n",
    "            self.data.append((context, target))  # Append the context and target pair to the data list\n",
    "\n",
    "    def __len__(self):  # Return the total number of samples in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):  # Get a single sample (context, target) at the given index\n",
    "        context, target = self.data[idx]  # Extract the context and target\n",
    "        context_ids = torch.tensor([word_to_ix[word] for word in context], dtype=torch.long)  # Convert context words to indices\n",
    "        target_id = torch.tensor(word_to_ix[target], dtype=torch.long)  # Convert target word to index\n",
    "        return context_ids, target_id  # Return the context and target as tensors\n",
    "\n",
    "# Instantiate the dataset and dataloader\n",
    "dataset = CBOWDataset(raw_text, CONTEXT_SIZE)  # Create the dataset\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)  # Create the data loader to handle batching and shuffling\n",
    "\n",
    "# Define the CBOW model\n",
    "class CBOW(nn.Module):  # Inherit from PyTorch's nn.Module\n",
    "    def __init__(self, vocab_size, embedding_dim):  # Initialize the model\n",
    "        super(CBOW, self).__init__()  # Call the parent class initializer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  # Layer to learn word embeddings\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)  # First linear layer to process embeddings\n",
    "        self.relu = nn.ReLU()  # Activation function to introduce non-linearity\n",
    "        self.linear2 = nn.Linear(128, vocab_size)  # Second linear layer to predict word probabilities\n",
    "\n",
    "    def forward(self, inputs):  # Define the forward pass of the model\n",
    "        embeds = self.embeddings(inputs).mean(dim=1)  # Get embeddings for inputs and average them (context representation)\n",
    "        out = self.linear1(embeds)  # Pass through the first linear layer\n",
    "        out = self.relu(out)  # Apply the ReLU activation\n",
    "        out = self.linear2(out)  # Pass through the second linear layer\n",
    "        return nn.functional.log_softmax(out, dim=1)  # Apply softmax and return log probabilities\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CBOW(vocab_size, EMBEDDING_DIM)  # Create an instance of the CBOW model\n",
    "loss_function = nn.NLLLoss()  # Negative Log-Likelihood Loss for classification\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)  # Stochastic Gradient Descent optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):  # Loop over the number of epochs\n",
    "    total_loss = 0  # Initialize total loss for this epoch\n",
    "    for context, target in data_loader:  # Loop over batches of data\n",
    "        optimizer.zero_grad()  # Reset gradients before backpropagation\n",
    "        output = model(context)  # Forward pass: get predictions\n",
    "        loss = loss_function(output, target)  # Compute the loss\n",
    "        loss.backward()  # Backpropagate the gradients\n",
    "        optimizer.step()  # Update the model weights\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")  # Print epoch loss\n",
    "\n",
    "# Test the model with a sample context\n",
    "context = raw_text[:4]  # Use the first 4 words as a sample context\n",
    "context_vector = torch.tensor([word_to_ix[word] for word in context], dtype=torch.long).unsqueeze(0)  # Convert to tensor\n",
    "prediction = model(context_vector)  # Get model predictions\n",
    "predicted_word = ix_to_word[torch.argmax(prediction).item()]  # Find the word with the highest probability\n",
    "print(f\"Context: {context}\")  # Print the input context\n",
    "print(f\"Predicted word: {predicted_word}\")  # Print the predicted word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab67f7f-b416-41cf-a2bb-19a10c0ca0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
